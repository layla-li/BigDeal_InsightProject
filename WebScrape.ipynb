{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import unicodecsv as csv\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_date(deal_end, description):\n",
    "    end_day = ''\n",
    "    end_month = ''\n",
    "    if deal_end not in description:\n",
    "        return None\n",
    "    else:\n",
    "        i = 0\n",
    "        while description[description.find(deal_end)+len(deal_end)+i].isdigit():\n",
    "            end_month += description[description.find(deal_end)+len(deal_end)+i]\n",
    "            i += 1\n",
    "        i += 1\n",
    "        while description[description.find(deal_end)+len(deal_end)+i].isdigit():\n",
    "            end_day += description[description.find(deal_end)+len(deal_end)+i]\n",
    "            i += 1\n",
    "            if description.find(deal_end)+len(deal_end)+i == len(description):\n",
    "                break\n",
    "        return end_month, end_day   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling(url, brand, page_max=100):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "\n",
    "    master = [['Brand', 'Title', 'Description', 'Posted_date', 'End_date', 'Comments_count', 'Bookmarks_count', 'Shares_count']]\n",
    "    page = 0\n",
    "    while True:\n",
    "        \n",
    "        check_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            try:\n",
    "                WebDriverWait(driver, 3).until(lambda driver: driver.execute_script(\"return document.body.scrollHeight;\")  > check_height)\n",
    "                check_height = driver.execute_script(\"return document.body.scrollHeight;\") \n",
    "            except:\n",
    "                 break\n",
    "\n",
    "        print('Started')\n",
    "        elements = driver.find_elements_by_class_name('mlist')\n",
    "\n",
    "        for element in elements:\n",
    "            temp = [brand]\n",
    "            deal_id = element.get_attribute('id')  #first find the deal_id\n",
    "                \n",
    "            #------ find title of deal\n",
    "            try:\n",
    "                title = element.find_element_by_class_name(\"txt\").text\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            temp.append(title)\n",
    "            \n",
    "            #------ find description of deal\n",
    "            des = element.find_element_by_class_name('brief').text\n",
    "            \n",
    "            for deal_ends in ['Deal ends on ', 'Deal ends ', 'Deal expires ', 'Coupon expires ']:\n",
    "                if not find_end_date(deal_ends, des):\n",
    "                    end_month = None\n",
    "                    end_day = None\n",
    "                    continue\n",
    "                else:\n",
    "                    end_month, end_day = find_end_date(deal_ends, des)\n",
    "                    try:\n",
    "                        end_month = int(end_month)\n",
    "                        end_day = int(end_day)\n",
    "                        if end_month > 12 or end_day > 31:\n",
    "                            end_month = None\n",
    "                            end_day = None\n",
    "                    except:\n",
    "                        pass\n",
    "                    break\n",
    "            \n",
    "            temp.append(des)\n",
    "            \n",
    "            #------ find time the deal was posted\n",
    "            try:\n",
    "                time = element.find_element_by_class_name('ib.published-date').text\n",
    "                time = time[0:-4]  #strip away 'Posted' and 'ago'\n",
    "                post_date = datetime.today()\n",
    "                if time[-4:] == 'days':\n",
    "                    post_date -= timedelta(days=int(time[0:-5]))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print ('===TIME NOT FOUND')\n",
    "                print ('===deal skipped')\n",
    "                continue\n",
    "            \n",
    "            temp.append(post_date.strftime(\"%m/%d/%Y\"))       \n",
    "            \n",
    "            #------ find time the deal was ended\n",
    "            \n",
    "            if end_month and end_day:\n",
    "                end_date = datetime(post_date.year, end_month, end_day)\n",
    "                end_date_saved = end_date.strftime(\"%m/%d/%Y\")\n",
    "            else:\n",
    "                end_date_saved = ''\n",
    "            \n",
    "                \n",
    "            temp.append(end_date_saved)\n",
    "            \n",
    "            stats = element.find_element_by_class_name(\"stat-count\")\n",
    "            stats_nums = stats.find_elements_by_class_name(\"j-count\")\n",
    "            \n",
    "            #------ find number of comments for the deal\n",
    "            num_comments = 0\n",
    "            try:\n",
    "                num_comments = stats_nums[0].text\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            temp.append(num_comments)\n",
    "            \n",
    "            #------ find number of bookmarks for the deal\n",
    "            num_bookmarks = 0\n",
    "            try:\n",
    "                num_bookmarks = stats_nums[1].text\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            temp.append(num_bookmarks)\n",
    "            \n",
    "            #------ find number of shares for the deal\n",
    "            num_shares = 0\n",
    "            try:\n",
    "                num_shares = stats_nums[2].text\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            temp.append(num_shares)\n",
    "            \n",
    "            #------ append to master list\n",
    "            master.append(temp)\n",
    "\n",
    "        try:\n",
    "            load = driver.find_element_by_class_name(\"next_link\")\n",
    "            page += 4\n",
    "            print(\"Finished {} pages\".format(page))\n",
    "\n",
    "            # See if the last page has been reached\n",
    "            page_num = driver.find_element_by_class_name('pages').find_element_by_class_name('current').text\n",
    "            \n",
    "            if page_num == str(page_max):\n",
    "                print ('Last page reached')\n",
    "                break\n",
    "            else:\n",
    "                load.click()          \n",
    "        except:\n",
    "            print (\"===Can't go to the next page\")\n",
    "            break \n",
    "            \n",
    "    return master\n",
    "\n",
    "def saveCSV(filename, data):\n",
    "    with open(filename, 'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_EsteeLauder = \"https://www.dealmoon.com/en/stores/estee-lauder?sort=relevance&exp=y\"\n",
    "data = crawling(url_EsteeLauder, 'Estee Lauder')\n",
    "saveCSV('EsteeLauder.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Carters = \"https://www.dealmoon.com/en/stores/carters?sort=relevance&exp=y\"\n",
    "data = crawling(url_Carters, 'Carters')\n",
    "saveCSV('Carters.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Oshkosh = \"https://www.dealmoon.com/en/stores/oshkosh-bgosh?sort=relevance&exp=y\"\n",
    "data = crawling(url_Oshkosh, 'Oshkosh')\n",
    "saveCSV('Oshkosh.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Gap = \"https://www.dealmoon.com/en/stores/gap?sort=relevance&exp=y\"\n",
    "data = crawling(url_Gap, 'Gap')\n",
    "saveCSV('Gap.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Clinique= \"https://www.dealmoon.com/en/stores/clinique?sort=relevance&exp=y\"\n",
    "data = crawling(url_Clinique, 'Clinique')\n",
    "saveCSV('Datasets/Clinique.csv', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_Loft= \"https://www.dealmoon.com/en/stores/loft-outlet?sort=relevance&exp=y\"\n",
    "data = crawling(url_Loft, 'Loft')\n",
    "saveCSV('Datasets/Loft.csv', data)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
